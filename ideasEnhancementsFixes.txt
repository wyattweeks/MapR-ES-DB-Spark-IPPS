
to finish project:

** reformat readme file

** remove sudo - not needed 

** add instructions / template for deployment with drill ports open to outside and public dns 
    - copy deploy template from paul's latest and make these changes and save - then ask him to publish as 'healthcare demo'
        - ports for drill
        - include .zip of .sh script

- add images and narrative from carol's blog to demo / readme doc
- learn query syntax for db-shell in #6 readme  
- get this working (add after #3 in the readme)
    # this part - Spark SQL - is an OPTION to the step above
    # in a sparate terminal window, start the spark shell with this command
    # add paramenter to spark version
    # need to automate the 'copy paste step below'
    #$ /opt/mapr/spark/spark-2.2.1/bin/spark-shell --master local[2]
    #copy paste  from the scripts/sparkshell file to query MapR-DB
# WW - FUTURE enhancement - add Drill query on Mapr-ES
# get cluster check script to work in demosetup.sh (currently commented out)


create multiple topics, so can run 3 consumers / 2 producers --- amend demosetup.sh to create additional topics AND readme script to 
launch 3 versions of producers/consumers, via run-time arguments in carol's original code (optionally)
this change will start with scala code in repo 
this change takes care or the to do note below
#               ### (TO DO NOTE FOR WW - experiment and future demo enhancement: ok to start multiple concurrent producers, interactively.  can start multiple concurrent consumers
#               ###                     interactively, and one will eventualld fail-over to the other/s with error e.g.:
#               ###                           (18/05/04 18:33:11 ERROR scheduler.JobScheduler: Error generating jobs for time 1525458790000 ms
#               ###                           org.apache.kafka.common.errors.UnknownTopicOrPartitionException: No such file or directory (2) Could not seek
#               ###                           2018-05-04 18:33:11,0013 ERROR StreamsListener fs/client/marlin/cc/listener/listenerimpl.cc:803 Thread: 14234 Seek called on unsubscribed partitions
#               ###                           2018-05-04 18:33:11,0013 ERROR StreamsListener fs/client/marlin/cc/listener/jni_listener.cc:756 Thread: 14234 Seek failed with err:2)
#               ###                           Exception in thread "main" org.apache.kafka.common.errors.UnknownTopicOrPartitionException: No such file or directory (2) Could not seek
#               ###                     1 - can i add a consumer group / partition and run these multiple consumer clients in that group, to demonstrate consumer client failover?
#               ###                     2 - also, could replicate stream and bring into demo/talk track)
#


is this needed?  possible?
## 2 - Consume and transform the streaming data with Spark Streaming and the Kafka API.
#
# This Spark Streaming consumer client does the following:
#      - consumes data from the MapR stream:topic @ /streams/paystream:payments using the Kafka API, then
#      - transforms the comma-delimited string consumed from the stream, into JSON format, then
#      - writes the JSON array, directly to the MapR-DB JSON table 'payments'.
# #########       (NOTE: future enhancement will use MapR-DB REST for write to table)


# Git Clone MapR-ES-DB-Spark-Payments project - done @  /public_data/demos_healthcare/MapR-ES-DB-Spark-Payments (git clone http://git.se.corp.maprtech.com/wweeks/MapR-ES-DB-Spark-Payments.git)
# Manually refresh when repo changes @ http://git.se.corp.maprtech.com/wweeks/MapR-ES-DB-Spark-Payments.git 
# maven rebuilds jars in /public_data/demos_healthcare/MapR-ES-DB-Spark-Payments/target
#    `mapr-es-db-spark-payment/target/mapr-es-db-spark-payment-1.0.jar`
#    `mapr-es-db-spark-payment/target/mapr-es-db-spark-payment-1.0-jar-with-dependencies.jar`
# (set auto refresh in future?)
# $ cd /public_data/demos_healthcare/MapR-ES-DB-Spark-Payments
# $ git pull 
# $ mvn clean install
